{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJEzefDFpyrV",
        "outputId": "4484f735-9898-4efc-d510-85b7cfa17330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN0H2IzqsG6u",
        "outputId": "c09afb77-18a3-4c5c-b663-e44ad2bc0573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1652253647.py:57: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
            "  df = pd.read_csv(\n",
            "/tmp/ipython-input-1652253647.py:57: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  df = pd.read_csv(\n",
            "/tmp/ipython-input-1652253647.py:57: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  df = pd.read_csv(\n",
            "/tmp/ipython-input-1652253647.py:71: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
            "  df = df.resample('1T').asfreq()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timesteps after cleaning: 2050108\n",
            "Windows: total=2049959, train=1434971, val=307494, test=307494\n",
            "Scaler fit on training region: (1435000, 1)\n",
            "Training NAR models for horizons 1..120 (paper used 120)\n",
            "\n",
            "=== NAR: Horizon h=1 ===\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "reproduce_paper_models.py\n",
        "\n",
        "Implements paper-like experiments:\n",
        " - NAR-style models: one scalar-output MLP per horizon (h = 1..H)\n",
        " - LSTM models: one scalar-output LSTM per horizon (h = 1..H)\n",
        "\n",
        "Notes:\n",
        " - The paper used Levenberg-Marquardt + Bayesian regularization for NAR (MATLAB).\n",
        "   This exact optimizer is not available in TF/Keras. We use Adam for NAR here.\n",
        " - LSTM uses AdaGrad (we use Keras Adagrad).\n",
        " - Training set sizes: NAR -> 5000, LSTM -> 100000 (defaults, can be changed).\n",
        " - WARNING: training 120 separate models can be expensive. You can restrict horizons\n",
        "   using H_MAX to experiment quickly.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# ===========================\n",
        "# USER PARAMETERS\n",
        "# ===========================\n",
        "DATA_PATH = \"/content/household_power_consumption.txt\"  # change if needed\n",
        "INPUT_COL = 'Global_active_power'\n",
        "PAST_STEPS = 30\n",
        "HORIZON = 120  # paper uses 120\n",
        "MAX_SAMPLES = None  # set to None to use all windows or e.g. 50000 to limit\n",
        "TEST_RATIO = 0.15\n",
        "VAL_RATIO = 0.15\n",
        "RANDOM_SEED = 42\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS_NAR = 100  # you can reduce for quick runs\n",
        "EPOCHS_LSTM = 50\n",
        "NAR_TRAIN_SAMPLES = 5000     # paper used ~5k for NAR\n",
        "LSTM_TRAIN_SAMPLES = 100000  # paper used ~100k for LSTM\n",
        "MODEL_DIR = \"paper_models\"   # saved models folder\n",
        "H_MAX = HORIZON  # set lower (e.g., 12) to speed up testing\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================\n",
        "# 0) Load & preprocess\n",
        "# ===========================\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\")\n",
        "\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\n",
        "    DATA_PATH,\n",
        "    sep=';',\n",
        "    parse_dates={'datetime': ['Date', 'Time']},\n",
        "    infer_datetime_format=True,\n",
        "    low_memory=False,\n",
        "    na_values=['?','']\n",
        ")\n",
        "df.sort_values('datetime', inplace=True)\n",
        "df.set_index('datetime', inplace=True)\n",
        "\n",
        "# convert target\n",
        "df[INPUT_COL] = pd.to_numeric(df[INPUT_COL], errors='coerce')\n",
        "# resample to 1-minute\n",
        "df = df.resample('1T').asfreq()\n",
        "\n",
        "# interpolate short gaps (paper used limit=60)\n",
        "df[INPUT_COL] = df[INPUT_COL].interpolate(method='time', limit=60)\n",
        "df = df.dropna(subset=[INPUT_COL])  # drop any remaining NaNs\n",
        "\n",
        "series = df[[INPUT_COL]].astype('float32')\n",
        "T = len(series)\n",
        "print(\"Timesteps after cleaning:\", T)\n",
        "\n",
        "# ===========================\n",
        "# 1) Build sliding windows indices\n",
        "# ===========================\n",
        "total_steps = len(series)\n",
        "max_start_idx = total_steps - (PAST_STEPS + HORIZON) + 1\n",
        "if max_start_idx <= 0:\n",
        "    raise ValueError(\"Not enough data for requested window sizes.\")\n",
        "\n",
        "n_windows = max_start_idx\n",
        "test_windows = int(np.ceil(TEST_RATIO * n_windows))\n",
        "val_windows = int(np.ceil(VAL_RATIO * n_windows))\n",
        "train_windows = n_windows - val_windows - test_windows\n",
        "print(f\"Windows: total={n_windows}, train={train_windows}, val={val_windows}, test={test_windows}\")\n",
        "\n",
        "start_indices = np.arange(0, n_windows, dtype=np.int64)\n",
        "if MAX_SAMPLES is not None:\n",
        "    start_indices = start_indices[:MAX_SAMPLES]\n",
        "    n_windows = len(start_indices)\n",
        "    test_windows = int(np.ceil(TEST_RATIO * n_windows))\n",
        "    val_windows = int(np.ceil(VAL_RATIO * n_windows))\n",
        "    train_windows = n_windows - val_windows - test_windows\n",
        "    print(f\"Limited windows -> total={n_windows}, train={train_windows}, val={val_windows}, test={test_windows}\")\n",
        "\n",
        "train_starts = start_indices[:train_windows]\n",
        "val_starts = start_indices[train_windows:train_windows + val_windows]\n",
        "test_starts = start_indices[train_windows + val_windows: train_windows + val_windows + test_windows]\n",
        "\n",
        "# ===========================\n",
        "# 2) Fit scaler on training inputs only\n",
        "# ===========================\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "# Fit on training inputs region (up to last input index present in training)\n",
        "last_input_idx = train_starts[-1] + PAST_STEPS - 1\n",
        "fit_slice = series.iloc[: last_input_idx + 1]\n",
        "scaler.fit(fit_slice.values)\n",
        "print(\"Scaler fit on training region:\", fit_slice.shape)\n",
        "\n",
        "# helper: build X (past) and scalar y for a given horizon h (1-based)\n",
        "def build_xy_scalar_h(series_values, starts, past_steps, h, scaler):\n",
        "    \"\"\"\n",
        "    returns X shape (N, past_steps, 1), y shape (N,)\n",
        "    h: forecast horizon in steps ahead (1..HORIZON)\n",
        "    \"\"\"\n",
        "    n = len(starts)\n",
        "    X = np.zeros((n, past_steps, series_values.shape[1]), dtype=np.float32)\n",
        "    y = np.zeros((n,), dtype=np.float32)\n",
        "    for i, s in enumerate(starts):\n",
        "        in_start = s\n",
        "        in_end = s + past_steps\n",
        "        out_idx = in_end + (h - 1)  # zero-based index of the scalar target\n",
        "        window_in = series_values[in_start:in_end]          # shape (past_steps, 1)\n",
        "        target = series_values[out_idx, 0]                  # scalar\n",
        "        X[i] = scaler.transform(window_in)\n",
        "        y[i] = scaler.transform(np.array([[target]]))[0,0]\n",
        "    return X, y\n",
        "\n",
        "series_values = series.values  # (T,1)\n",
        "\n",
        "# ===========================\n",
        "# 3) Helper to create and train NAR model for one horizon\n",
        "# ===========================\n",
        "def make_nar_model(past_steps):\n",
        "    # Single hidden layer, 40 neurons, sigmoid activation, linear output\n",
        "    model = Sequential([\n",
        "        Input(shape=(past_steps,1)),\n",
        "        Flatten(),\n",
        "        Dense(40, activation='sigmoid'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    # NOTE: paper used Levenberg-Marquardt + Bayesian reg (MATLAB). Not available in Keras.\n",
        "    # We use Adam here for practicality. Replace optimizer below if desired.\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mae', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# ===========================\n",
        "# 4) Helper to create and train LSTM model for one horizon\n",
        "# ===========================\n",
        "def make_lstm_model(past_steps):\n",
        "    # LSTM with 50 units and softsign activation, linear output\n",
        "    model = Sequential([\n",
        "        Input(shape=(past_steps,1)),\n",
        "        LSTM(50, activation='softsign'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=1e-2), loss='mae', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# ===========================\n",
        "# 5) Training loop: NAR models (one per horizon)\n",
        "# ===========================\n",
        "# To save time you can restrict H_MAX (e.g., 12 or 24). Set H_MAX = HORIZON for full run.\n",
        "H_MAX = min(H_MAX, HORIZON)\n",
        "print(f\"Training NAR models for horizons 1..{H_MAX} (paper used 120)\")\n",
        "nar_results = {}\n",
        "\n",
        "for h in range(1, H_MAX + 1):\n",
        "    print(f\"\\n=== NAR: Horizon h={h} ===\")\n",
        "    # build datasets (train/val/test) for this horizon\n",
        "    X_tr, y_tr = build_xy_scalar_h(series_values, train_starts, PAST_STEPS, h, scaler)\n",
        "    X_val, y_val = build_xy_scalar_h(series_values, val_starts, PAST_STEPS, h, scaler)\n",
        "    X_te, y_te = build_xy_scalar_h(series_values, test_starts, PAST_STEPS, h, scaler)\n",
        "\n",
        "    # Optionally limit training samples to match paper's approx regime\n",
        "    if NAR_TRAIN_SAMPLES is not None and NAR_TRAIN_SAMPLES < X_tr.shape[0]:\n",
        "        X_tr_use = X_tr[:NAR_TRAIN_SAMPLES]\n",
        "        y_tr_use = y_tr[:NAR_TRAIN_SAMPLES]\n",
        "    else:\n",
        "        X_tr_use = X_tr; y_tr_use = y_tr\n",
        "\n",
        "    model = make_nar_model(PAST_STEPS)\n",
        "    fname = os.path.join(MODEL_DIR, f\"nar_h{h:03d}.h5\")\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
        "        ModelCheckpoint(fname, monitor='val_loss', save_best_only=True, verbose=0)\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_tr_use, y_tr_use,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=EPOCHS_NAR,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks,\n",
        "        shuffle=False,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # evaluate on test set\n",
        "    y_pred = model.predict(X_te).reshape(-1)\n",
        "    # inverse scale\n",
        "    y_te_inv = scaler.inverse_transform(y_te.reshape(-1,1)).reshape(-1)\n",
        "    y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1,1)).reshape(-1)\n",
        "    mae = mean_absolute_error(y_te_inv, y_pred_inv)\n",
        "    print(f\"NAR h={h} test MAE: {mae:.4f} (units same as original series)\")\n",
        "    nar_results[h] = {'model_path': fname, 'test_mae': float(mae)}\n",
        "\n",
        "# Save summary\n",
        "import json\n",
        "with open(os.path.join(MODEL_DIR, \"nar_summary.json\"), \"w\") as f:\n",
        "    json.dump(nar_results, f, indent=2)\n",
        "\n",
        "# ===========================\n",
        "# 6) Training loop: LSTM models (one per horizon)\n",
        "# ===========================\n",
        "# LSTM training uses larger training set by default. Be mindful of time.\n",
        "H_MAX_LSTM = H_MAX  # you can set separately\n",
        "print(f\"\\nTraining LSTM models for horizons 1..{H_MAX_LSTM}\")\n",
        "lstm_results = {}\n",
        "\n",
        "for h in range(1, H_MAX_LSTM + 1):\n",
        "    print(f\"\\n=== LSTM: Horizon h={h} ===\")\n",
        "    X_tr, y_tr = build_xy_scalar_h(series_values, train_starts, PAST_STEPS, h, scaler)\n",
        "    X_val, y_val = build_xy_scalar_h(series_values, val_starts, PAST_STEPS, h, scaler)\n",
        "    X_te, y_te = build_xy_scalar_h(series_values, test_starts, PAST_STEPS, h, scaler)\n",
        "\n",
        "    # limit training samples to match paper guidance for LSTM\n",
        "    if LSTM_TRAIN_SAMPLES is not None and LSTM_TRAIN_SAMPLES < X_tr.shape[0]:\n",
        "        X_tr_use = X_tr[:LSTM_TRAIN_SAMPLES]\n",
        "        y_tr_use = y_tr[:LSTM_TRAIN_SAMPLES]\n",
        "    else:\n",
        "        X_tr_use = X_tr; y_tr_use = y_tr\n",
        "\n",
        "    model = make_lstm_model(PAST_STEPS)\n",
        "    fname = os.path.join(MODEL_DIR, f\"lstm_h{h:03d}.h5\")\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
        "        ModelCheckpoint(fname, monitor='val_loss', save_best_only=True, verbose=0)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_tr_use, y_tr_use,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=EPOCHS_LSTM,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks,\n",
        "        shuffle=False,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_te).reshape(-1)\n",
        "    y_te_inv = scaler.inverse_transform(y_te.reshape(-1,1)).reshape(-1)\n",
        "    y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1,1)).reshape(-1)\n",
        "    mae = mean_absolute_error(y_te_inv, y_pred_inv)\n",
        "    print(f\"LSTM h={h} test MAE: {mae:.4f}\")\n",
        "    lstm_results[h] = {'model_path': fname, 'test_mae': float(mae)}\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"lstm_summary.json\"), \"w\") as f:\n",
        "    json.dump(lstm_results, f, indent=2)\n",
        "\n",
        "print(\"Done. Models & summaries saved in:\", MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq5SrYUVqKgG"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/content/household_power_consumption.txt\"\n",
        "print(\"Dataset found:\", DATA_PATH)\n",
        "\n",
        "# ============================\n",
        "# Imports\n",
        "# ============================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# ============================\n",
        "# Parameters\n",
        "# ============================\n",
        "TXT_FILENAME = DATA_PATH\n",
        "INPUT_COL = 'Global_active_power'\n",
        "PAST_STEPS = 30\n",
        "FUTURE_STEPS = 120\n",
        "TEST_RATIO = 0.15\n",
        "VAL_RATIO = 0.15\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 50\n",
        "RANDOM_SEED = 42\n",
        "MODEL_SAVE_PATH = 'best_simple_mlp.h5'\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# ============================\n",
        "# Load & preprocess data\n",
        "# ============================\n",
        "df = pd.read_csv(\n",
        "    TXT_FILENAME,\n",
        "    sep=';',\n",
        "    parse_dates={'datetime': ['Date', 'Time']},\n",
        "    na_values=['?', ''],\n",
        "    low_memory=False\n",
        ")\n",
        "\n",
        "df.sort_values('datetime', inplace=True)\n",
        "df.set_index('datetime', inplace=True)\n",
        "\n",
        "df[INPUT_COL] = pd.to_numeric(df[INPUT_COL], errors='coerce')\n",
        "df = df.resample('1T').asfreq()\n",
        "df[INPUT_COL] = df[INPUT_COL].interpolate(method='time', limit=60)\n",
        "df = df.dropna(subset=[INPUT_COL])\n",
        "\n",
        "series = df[[INPUT_COL]].astype('float32')\n",
        "series_values = series.values\n",
        "\n",
        "# ============================\n",
        "# Sliding windows\n",
        "# ============================\n",
        "total_steps = len(series)\n",
        "n_windows = total_steps - (PAST_STEPS + FUTURE_STEPS) + 1\n",
        "\n",
        "test_windows = int(np.ceil(TEST_RATIO * n_windows))\n",
        "val_windows = int(np.ceil(VAL_RATIO * n_windows))\n",
        "train_windows = n_windows - test_windows - val_windows\n",
        "\n",
        "starts = np.arange(n_windows)\n",
        "train_starts = starts[:train_windows]\n",
        "val_starts = starts[train_windows:train_windows + val_windows]\n",
        "test_starts = starts[train_windows + val_windows:]\n",
        "\n",
        "# ============================\n",
        "# Scaling (train-only)\n",
        "# ============================\n",
        "scaler = MinMaxScaler()\n",
        "last_train_idx = train_starts[-1] + PAST_STEPS\n",
        "scaler.fit(series_values[:last_train_idx])\n",
        "\n",
        "def build_xy(starts):\n",
        "    X = np.zeros((len(starts), PAST_STEPS, 1))\n",
        "    Y = np.zeros((len(starts), FUTURE_STEPS))\n",
        "    for i, s in enumerate(starts):\n",
        "        X[i] = scaler.transform(series_values[s:s+PAST_STEPS])\n",
        "        Y[i] = scaler.transform(series_values[s+PAST_STEPS:s+PAST_STEPS+FUTURE_STEPS]).reshape(-1)\n",
        "    return X, Y\n",
        "\n",
        "X_train, Y_train = build_xy(train_starts)\n",
        "X_val, Y_val = build_xy(val_starts)\n",
        "X_test, Y_test = build_xy(test_starts)\n",
        "\n",
        "# ============================\n",
        "# Build MLP\n",
        "# ============================\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(PAST_STEPS, 1)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(FUTURE_STEPS, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss='mae'\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ============================\n",
        "# Train\n",
        "# ============================\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=6, restore_best_weights=True),\n",
        "    ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Predict & inverse scale\n",
        "# ============================\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "Y_test_inv = scaler.inverse_transform(Y_test.reshape(-1,1)).reshape(-1, FUTURE_STEPS)\n",
        "Y_pred_inv = scaler.inverse_transform(Y_pred.reshape(-1,1)).reshape(-1, FUTURE_STEPS)\n",
        "\n",
        "# ============================\n",
        "# Metrics per horizon\n",
        "# ============================\n",
        "mae_scores = np.zeros(FUTURE_STEPS)\n",
        "mse_scores = np.zeros(FUTURE_STEPS)\n",
        "r2_scores  = np.zeros(FUTURE_STEPS)\n",
        "\n",
        "for i in range(FUTURE_STEPS):\n",
        "    mae_scores[i] = mean_absolute_error(Y_test_inv[:, i], Y_pred_inv[:, i])\n",
        "    mse_scores[i] = mean_squared_error(Y_test_inv[:, i], Y_pred_inv[:, i])\n",
        "    r2_scores[i]  = r2_score(Y_test_inv[:, i], Y_pred_inv[:, i])\n",
        "\n",
        "error_variance = np.var(Y_test_inv - Y_pred_inv, axis=0)\n",
        "\n",
        "# ============================\n",
        "# PLOTS (MATCH FRIEND)\n",
        "# ============================\n",
        "\n",
        "# MAE\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(range(1, FUTURE_STEPS+1), mae_scores, label='MLP - MAE')\n",
        "plt.xlabel('Forecast Span [minutes]')\n",
        "plt.ylabel('Mean Absolute Error (MAE)')\n",
        "plt.title('MAE for Multi-Step Forecasting (MLP)')\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# MSE\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(range(1, FUTURE_STEPS+1), mse_scores, label='MLP - MSE')\n",
        "plt.xlabel('Forecast Span [minutes]')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('MSE for Multi-Step Forecasting (MLP)')\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# R²\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(range(1, FUTURE_STEPS+1), r2_scores, label='MLP - R²')\n",
        "plt.xlabel('Forecast Span [minutes]')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('R² for Multi-Step Forecasting (MLP)')\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Error Variance\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(range(1, FUTURE_STEPS+1),\n",
        "         error_variance,\n",
        "         label='MLP - Error Variance',\n",
        "         marker='o',\n",
        "         markersize=4)\n",
        "\n",
        "plt.xlabel('Forecast Span [minutes]')\n",
        "plt.ylabel('Error Variance')\n",
        "plt.title('Error Variance for Multi-Step Forecasting (MLP)')\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# ============================\n",
        "# Save\n",
        "# ============================\n",
        "np.savez(\n",
        "    'simple_mlp_results_full.npz',\n",
        "    mae=mae_scores,\n",
        "    mse=mse_scores,\n",
        "    r2=r2_scores,\n",
        "    error_variance=error_variance,\n",
        "    Y_test=Y_test_inv,\n",
        "    Y_pred=Y_pred_inv\n",
        ")\n",
        "\n",
        "print(\"✅ DONE — All graphs generated (MAE, MSE, R², Error Variance)\")\n",
        "print(\"Model saved to:\", MODEL_SAVE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "O_wtoJBNp7cg",
        "outputId": "78c675f7-1945-4e85-b9ae-2aa3ec953fca"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'n_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3592181549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Paper config: one hidden layer with 50 memory cells (units), softsign activation, linear output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m lstm_model = Sequential([\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAST_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# n_features should be 1 for your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softsign'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# 50 memory cells as in paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# optional dropout to reduce overfitting (tune or remove)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_features' is not defined"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# LSTM model for multi-step forecasting\n",
        "# Re-uses arrays: X_train, Y_train, X_val, Y_val, X_test, Y_test, scaler\n",
        "# and constants: PAST_STEPS, FUTURE_STEPS, BATCH_SIZE, EPOCHS, RANDOM_SEED\n",
        "# ---------------------------\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "LSTM_MODEL_SAVE_PATH = 'best_lstm.h5'\n",
        "\n",
        "# reproducibility (already set earlier, but safe to set again)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# clear session to avoid clutter\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Build LSTM model\n",
        "# Paper config: one hidden layer with 50 memory cells (units), softsign activation, linear output.\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(PAST_STEPS, n_features)),         # n_features should be 1 for your dataset\n",
        "    LSTM(50, activation='softsign'),              # 50 memory cells as in paper\n",
        "    # optional dropout to reduce overfitting (tune or remove)\n",
        "    Dropout(0.1),\n",
        "    Dense(FUTURE_STEPS, activation='linear')      # predict all FUTURE_STEPS in one shot\n",
        "])\n",
        "\n",
        "# Compile with Adagrad optimizer (paper used ADAGRAD). Use MAE loss to match MLP experiment.\n",
        "lstm_model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=1e-2),\n",
        "                   loss='mae',\n",
        "                   metrics=['mae'])\n",
        "\n",
        "lstm_model.summary()\n",
        "\n",
        "# Callbacks: early stopping and checkpointing (save best by val_loss)\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(LSTM_MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Fit model\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation - MAE per horizon\n",
        "# ---------------------------\n",
        "print(\"Predicting on test set (LSTM)...\")\n",
        "Y_pred_test_lstm = lstm_model.predict(X_test)  # shape (N, FUTURE_STEPS)\n",
        "\n",
        "# Inverse transform outputs back to original scale\n",
        "Y_test_inv_lstm = scaler.inverse_transform(Y_test.reshape(-1, 1)).reshape(-1, FUTURE_STEPS)\n",
        "Y_pred_inv_lstm = scaler.inverse_transform(Y_pred_test_lstm.reshape(-1, 1)).reshape(-1, FUTURE_STEPS)\n",
        "\n",
        "# compute MAE per horizon\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae_per_horizon_lstm = []\n",
        "for h in range(FUTURE_STEPS):\n",
        "    mae_h = mean_absolute_error(Y_test_inv_lstm[:, h], Y_pred_inv_lstm[:, h])\n",
        "    mae_per_horizon_lstm.append(mae_h)\n",
        "\n",
        "overall_mae_lstm = mean_absolute_error(Y_test_inv_lstm.reshape(-1), Y_pred_inv_lstm.reshape(-1))\n",
        "print(f\"LSTM overall test MAE (all horizons): {overall_mae_lstm:.4f} kW\")\n",
        "\n",
        "# Plot MAE vs horizon (same plotting style as MLP)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.arange(1, FUTURE_STEPS + 1), mae_per_horizon_lstm, marker='o', markersize=3)\n",
        "plt.xlabel('Minutes ahead')\n",
        "plt.ylabel('MAE (kW)')\n",
        "plt.title('MAE vs Forecast Horizon (LSTM)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot sample predictions (like MLP)\n",
        "n_samples_plot = 3\n",
        "fig, axes = plt.subplots(n_samples_plot, 1, figsize=(10, 3*n_samples_plot))\n",
        "for i in range(n_samples_plot):\n",
        "    ax = axes[i]\n",
        "    idx = i if i < X_test.shape[0] else 0\n",
        "    input_window = scaler.inverse_transform(X_test[idx].reshape(-1, 1)).reshape(-1)\n",
        "    actual_future = Y_test_inv_lstm[idx]\n",
        "    pred_future = Y_pred_inv_lstm[idx]\n",
        "    t_input = np.arange(-PAST_STEPS, 0)\n",
        "    t_future = np.arange(0, FUTURE_STEPS)\n",
        "    ax.plot(t_input, input_window, label='Input (past 30 min)', marker='o')\n",
        "    ax.plot(t_future, actual_future, label='Actual future (next 120 min)')\n",
        "    ax.plot(t_future, pred_future, label='Predicted future (LSTM)')\n",
        "    ax.axvline(-0.5, color='k', linestyle='--')\n",
        "    ax.set_xlabel('Minutes (relative)')\n",
        "    ax.set_ylabel('Global_active_power (kW)')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save results\n",
        "np.savez('lstm_results.npz',\n",
        "         mae_per_horizon=np.array(mae_per_horizon_lstm),\n",
        "         Y_test=Y_test_inv_lstm,\n",
        "         Y_pred=Y_pred_inv_lstm)\n",
        "\n",
        "print(\"Done. LSTM model saved to:\", LSTM_MODEL_SAVE_PATH)\n",
        "print(\"Results saved to lstm_results.npz\")\n",
        "\n",
        "# ----------------------------------------------\n",
        "# EXTRA: Three plots for report (MAE, MSE, VAR)\n",
        "# ----------------------------------------------\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ============= 1) MAE per forecast horizon =============\n",
        "mae_per_horizon = []\n",
        "for h in range(FUTURE_STEPS):\n",
        "    mae = mean_absolute_error(Y_test_inv_lstm[:, h], Y_pred_inv_lstm[:, h])\n",
        "    mae_per_horizon.append(mae)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, FUTURE_STEPS+1), mae_per_horizon, marker='o', markersize=3)\n",
        "plt.xlabel(\"Forecast Span [minutes]\")\n",
        "plt.ylabel(\"MAE (kWh)\")\n",
        "plt.title(\"Mean Absolute Error (MAE) vs Forecast Span (LSTM)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ============= 2) MSE per forecast horizon =============\n",
        "mse_per_horizon = []\n",
        "for h in range(FUTURE_STEPS):\n",
        "    mse = mean_squared_error(Y_test_inv_lstm[:, h], Y_pred_inv_lstm[:, h])\n",
        "    mse_per_horizon.append(mse)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, FUTURE_STEPS+1), mse_per_horizon, marker='o', markersize=3)\n",
        "plt.xlabel(\"Forecast Span [minutes]\")\n",
        "plt.ylabel(\"MSE (kWh)\")\n",
        "plt.title(\"Mean Squared Error (MSE) vs Forecast Span (LSTM)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ============= 3) Error Variance per horizon =============\n",
        "error_variance = []\n",
        "for h in range(FUTURE_STEPS):\n",
        "    errors = Y_test_inv_lstm[:, h] - Y_pred_inv_lstm[:, h]\n",
        "    error_variance.append(np.var(errors))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, FUTURE_STEPS+1), error_variance, marker='o', markersize=3)\n",
        "plt.xlabel(\"Forecast Span [minutes]\")\n",
        "plt.ylabel(\"Error Variance\")\n",
        "plt.title(\"Error Variance vs Forecast Span (LSTM)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Generated MAE, MSE, and Error Variance plots for report.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
