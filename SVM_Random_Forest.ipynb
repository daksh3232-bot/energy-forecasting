{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install ucimlrepo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from ucimlrepo import fetch_ucirepo\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.multioutput import MultiOutputRegressor\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "\n",
                "# Fetch dataset\n",
                "ds = fetch_ucirepo(id=235)\n",
                "X_raw = ds.data.features\n",
                "y_raw = ds.data.targets\n",
                "df = pd.concat([X_raw, y_raw], axis=1)\n",
                "\n",
                "# Preprocessing\n",
                "df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')\n",
                "df = df[['Global_active_power']].fillna(df['Global_active_power'].mean())\n",
                "\n",
                "# Sliding Window\n",
                "def create_sliding_window(df, window_size=30, forecast_horizon=120):\n",
                "    X, y = [], []\n",
                "    data = df['Global_active_power'].values\n",
                "    for i in range(window_size, len(data) - forecast_horizon):\n",
                "        X.append(data[i-window_size:i])\n",
                "        y.append(data[i+1:i+forecast_horizon+1])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "window_size = 30\n",
                "forecast_horizon = 120\n",
                "X, y = create_sliding_window(df, window_size, forecast_horizon)\n",
                "\n",
                "# Split & Scale\n",
                "# Using the same split logic: last 20% is test, but we only use 5000 samples for training from the train split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
                "\n",
                "# Limit training size to 5000 samples as per paper/original notebook to manage compute time\n",
                "X_train = X_train[:5000]\n",
                "y_train = y_train[:5000]\n",
                "\n",
                "# Standardization is crucial for SVM\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"Data prepared. Train shape:\", X_train_scaled.shape, \"Test shape:\", X_test_scaled.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Models\n",
                "\n",
                "# 1. SVM (SVR)\n",
                "# SVR does not support multi-output natively, so we wrap it in MultiOutputRegressor.\n",
                "# This means it will fit 120 individual SVR models (one for each future time step).\n",
                "# RBF kernel is generally best for non-linear time series.\n",
                "print(\"Initializing SVM (MultiOutput SVR)...\")\n",
                "svm_model = MultiOutputRegressor(SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1), n_jobs=-1)\n",
                "\n",
                "# 2. Random Forest\n",
                "# Random Forest natively supports multi-output regression.\n",
                "# However, to be consistent with the paper's likely approach of '1 model per minute' (implied by the error variance plotting per step),\n",
                "# and the previous notebook's use of MultiOutputRegressor for DecisionTrees, we can wrap it too.\n",
                "# Wrapping it means we get 120 independent forests. Using native RF means we get one forest predicting 120 outputs.\n",
                "# Native is much faster and often better. But strictly '1 model per minute' implies MultiOutputWrapper.\n",
                "# Let's use MultiOutputRegressor to strictly follow the '1 model per minute' paradigm mentioned in the previous notebook's comments.\n",
                "print(\"Initializing Random Forest (MultiOutput)... \")\n",
                "rf_model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1), n_jobs=-1)\n",
                "\n",
                "print(\"Training SVM (this may take a few minutes for 5000 samples x 120 outputs)...\")\n",
                "svm_model.fit(X_train_scaled, y_train)\n",
                "print(\"SVM Trained.\")\n",
                "\n",
                "print(\"Training Random Forest...\")\n",
                "rf_model.fit(X_train_scaled, y_train)\n",
                "print(\"Random Forest Trained.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predictions\n",
                "print(\"Predicting SVM...\")\n",
                "y_pred_svm = svm_model.predict(X_test_scaled)\n",
                "print(\"Predicting Random Forest...\")\n",
                "y_pred_rf = rf_model.predict(X_test_scaled)\n",
                "\n",
                "# Evaluation Metrics (Global)\n",
                "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
                "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
                "r2_svm = r2_score(y_test, y_pred_svm)\n",
                "\n",
                "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
                "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
                "r2_rf = r2_score(y_test, y_pred_rf)\n",
                "\n",
                "print(f\"SVM - MAE: {mae_svm:.4f}, MSE: {mse_svm:.4f}, R2: {r2_svm:.4f}\")\n",
                "print(f\"Random Forest - MAE: {mae_rf:.4f}, MSE: {mse_rf:.4f}, R2: {r2_rf:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "plt.figure(figsize=(18, 12))\n",
                "\n",
                "# 1. Error Variance per forecast step\n",
                "plt.subplot(2, 2, 1)\n",
                "error_variance_svm = np.var(y_test - y_pred_svm, axis=0)\n",
                "error_variance_rf = np.var(y_test - y_pred_rf, axis=0)\n",
                "plt.plot(np.arange(1, forecast_horizon + 1), error_variance_svm, label='SVM', marker='.')\n",
                "plt.plot(np.arange(1, forecast_horizon + 1), error_variance_rf, label='Random Forest', marker='.')\n",
                "plt.title('Error Variance per Forecast Step (Lower is better)')\n",
                "plt.xlabel('Forecast Span [minutes]')\n",
                "plt.ylabel('Variance')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "# 2. MAE per step\n",
                "plt.subplot(2, 2, 2)\n",
                "plt.plot(np.mean(np.abs(y_test - y_pred_svm), axis=0), label='SVM')\n",
                "plt.plot(np.mean(np.abs(y_test - y_pred_rf), axis=0), label='Random Forest')\n",
                "plt.title('MAE per Forecast Step (Lower is better)')\n",
                "plt.xlabel('Forecast Span [minutes]')\n",
                "plt.ylabel('MAE')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "# 3. MSE per step\n",
                "plt.subplot(2, 2, 3)\n",
                "plt.plot(np.mean((y_test - y_pred_svm)**2, axis=0), label='SVM')\n",
                "plt.plot(np.mean((y_test - y_pred_rf)**2, axis=0), label='Random Forest')\n",
                "plt.title('MSE per Forecast Step (Lower is better)')\n",
                "plt.xlabel('Forecast Span [minutes]')\n",
                "plt.ylabel('MSE')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "# 4. R2 per step\n",
                "plt.subplot(2, 2, 4)\n",
                "r2_svm_steps = [r2_score(y_test[:, i], y_pred_svm[:, i]) for i in range(forecast_horizon)]\n",
                "r2_rf_steps = [r2_score(y_test[:, i], y_pred_rf[:, i]) for i in range(forecast_horizon)]\n",
                "plt.plot(r2_svm_steps, label='SVM')\n",
                "plt.plot(r2_rf_steps, label='Random Forest')\n",
                "plt.title('R2 per Forecast Step (Higher is better)')\n",
                "plt.xlabel('Forecast Span [minutes]')\n",
                "plt.ylabel('R2 Score')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}